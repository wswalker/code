{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Base Inputs\n",
    "########################################\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "pd.set_option('display.max_rows',2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Importing Parking Tickets - TSV Method\n",
    "########################################\n",
    "'''\n",
    "TSV can be downloaded from:\n",
    "  https://data.lacity.org/A-Well-Run-City/Parking-Citations/wjz9-h9np\n",
    "  \n",
    "This code needs the raw data to be named 'Parking_Citations.tsv'\n",
    "and to be in the same folder as the jupyter notebook\n",
    "'''\n",
    "\n",
    "df_raw = pd.read_csv('Parking_Citations.tsv',sep='\\t',header=0)\n",
    "\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "########################################\n",
    "# Importing Parking Tickets - API Method\n",
    "########################################\n",
    "import requests\n",
    "import io\n",
    "\n",
    "'''\n",
    "Requests.get() can only pull 50K rows at a time.\n",
    "Because this data set is so large, it's better to\n",
    "download the tsv from lacity.org than using the\n",
    "API method.  It will take over a hundred loop \n",
    "iterations to get all of the data using API\n",
    "'''\n",
    "\n",
    "#This is the first iteration of pulling 50K rows to create the dataframe\n",
    "apiData = requests.get('https://data.lacity.org/resource/8yfh-4gug.json?$limit=50000').content\n",
    "df_api_raw = pd.DataFrame.from_dict(eval(apiData),orient='columns')\n",
    "\n",
    "'''\n",
    "The last time I looked, the data set had just under 8M records\n",
    "8M records would require 159 additional iterations of pulling 50K rows\n",
    "You can find out how many rows of data are in the data set in the below link:\n",
    "  https://data.lacity.org/A-Well-Run-City/Parking-Citations/wjz9-h9np\n",
    "'''\n",
    "remaining_iterations_needed = 159\n",
    "\n",
    "#will continue to pull iterations\n",
    "for i in range(2,remaining_iterations_needed+2):\n",
    "  offset=50000*i\n",
    "  offset_string = str(offset)\n",
    "  apiData = requests.get('https://data.lacity.org/resource/8yfh-4gug.json?$limit=50000&$offset=%s'%(offset_string)).content\n",
    "  for_loop_df=pd.DataFrame.from_dict(eval(apiData),orient='columns')\n",
    "  df_api_raw=df_api_raw.append(for_loop_df,ignore_index=True,sort=True)\n",
    "\n",
    "'''\n",
    "The next two commented rows of code are to ensure the data \n",
    "being pulled is distinct.  If the count of ticket numbers\n",
    "equals the unique count of tickets then the data is pulling\n",
    "over correctly.\n",
    "'''\n",
    "#print(df_api_raw.ticket_number.count())\n",
    "#print(df_api_raw.ticket_number.nunique())\n",
    "print(df_api_raw.count())\n",
    "df_api_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Importing Agency Codes\n",
    "########################################\n",
    "'''\n",
    "TSV can be downloaded from:\n",
    "  https://data.lacity.org/A-Well-Run-City/Parking-Citations/wjz9-h9np\n",
    "  \n",
    "This code needs the raw data to be named 'agency_codes.tsv'\n",
    "and to be in the same folder as the jupyter notebook\n",
    "'''\n",
    "\n",
    "agency_df_raw = pd.read_csv('agency_codes.tsv',sep='\\t',header=0)\n",
    "\n",
    "agency_df=agency_df_raw\n",
    "\n",
    "agency_df.columns=['Agency','agency_name','agency_shortname']\n",
    "agency_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Cleaning Data\n",
    "########################################\n",
    "df = df_raw\n",
    "\n",
    "########################################\n",
    "# Adding Data\n",
    "########################################\n",
    "df['one']=1 #used instead of ticket number to count rows\n",
    "df = pd.merge(df,agency_df,how='left',on=['Agency']) #adding agency name\n",
    "\n",
    "########################################\n",
    "# Removing Unnecessary Data\n",
    "########################################\n",
    "df = df[df['Issue Date'].notnull()] #removing anything with an unknown issue date\n",
    "df = df[df['Fine amount'].notnull()] #removing anything with an unknown fine amount\n",
    "del df['VIN'] #If these are actually VINs, I'm not sure if they should be released publicly in the first place\n",
    "del df['Meter Id'] #no need for this\n",
    "del df['Marked Time'] #not sure what this is. Don't need it\n",
    "del df['Ticket number'] # no need to keep this right now since it's just a unique value per now\n",
    "del df['Route'] #don't know how I'd use this\n",
    "del df['Violation code'] #I can use the violation name instead\n",
    "del df['Plate Expiry Date'] #I'm going to leave this out to keep things simple\n",
    "\n",
    "#Lat/Lon is in US Feet coordinates according to the NAD1983StatePlaneCaliforniaVFIPS0405_Feet projection\n",
    "#Removing for now\n",
    "del df['Latitude']\n",
    "del df['Longitude']\n",
    "\n",
    "del df['Location'] #I would need a serious geocoding package to make use of this partial address --might be able to use the Google Geocoding API\n",
    "del df['Agency'] #No need for Agency Code now\n",
    "del df['agency_shortname'] #No need for agency shortname right now\n",
    "\n",
    "########################################\n",
    "# Replacing Nulls\n",
    "########################################\n",
    "\n",
    "#this part is going to make it easier when I do the group bys later so we can include the nulls\n",
    "df['Issue time'].fillna(value = -999,inplace = True,axis=0) #replacing nulls with -999\n",
    "df['RP State Plate'].fillna(value = 'Unknown',inplace = True,axis=0) #replacing nulls with 'Unknown'\n",
    "df['Make'].fillna(value = 'Unknown',inplace = True,axis=0) #replacing nulls with 'Unknown'\n",
    "df['Body Style'].fillna(value = 'Unknown',inplace = True,axis=0) #replacing nulls with 'Unknown'\n",
    "df['Color'].fillna(value = 'Unknown',inplace = True,axis=0) #replacing nulls with 'Unknown'\n",
    "df['Violation Description'].fillna(value = 'Unknown',inplace = True,axis=0) #replacing nulls with 'Unknown'\n",
    "df['agency_name'].fillna(value = 'Unknown',inplace = True,axis=0) #replacing nulls with 'Unknown'\n",
    "\n",
    "########################################\n",
    "# Time\n",
    "########################################\n",
    "\n",
    "# Time\n",
    "df['Issue time']=df['Issue time'].astype(int)\n",
    "df['Issue time']=df['Issue time'].apply('{0:0>4}'.format) #nulls will come out as '-999'\n",
    "df['Issue time']=df['Issue time'].astype(str)\n",
    "\n",
    "df['issue_hour']=df['Issue time'].str[:2] #nulls will show up as '-9'\n",
    "\n",
    "'''The next two lines of commented code are for \n",
    "minute-level information.  I'm excluding it for now'''\n",
    "#df['issue_minute']=df['Issue time'].str[2:4]\n",
    "#df['issue_h_m']=df['issue_hour']+':'+df['issue_minute']+':00'\n",
    "\n",
    "########################################\n",
    "# Last Deletes for Space\n",
    "########################################\n",
    "'''\n",
    "These fields were deleted to save space.\n",
    "Tableau Public can only handle 1M rows\n",
    "'''\n",
    "\n",
    "del df['Issue time'] #too much granularity\n",
    "del df['RP State Plate'] #mostly california\n",
    "del df['Make'] #I can do without this\n",
    "del df['Body Style']\n",
    "del df['Color'] \n",
    "del df['agency_name']\n",
    "########################################\n",
    "# Renaming Columns\n",
    "########################################\n",
    "df.rename(\n",
    "        columns ={\n",
    "            'Issue Date':'issue_date'\n",
    "            ,'Issue time':'issue_time'\n",
    "            ,'RP State Plate':'state_plate'\n",
    "            ,'Body Style':'veh_body'\n",
    "            ,'Violation Description':'violation'\n",
    "            ,'Fine amount':'fine'\n",
    "                }\n",
    "        ,inplace = True)\n",
    "\n",
    "print('Total Rows: %d' % (df.one.sum()))\n",
    "print('')\n",
    "print(df.count())\n",
    "print('')\n",
    "print('')\n",
    "print(df.info())\n",
    "print('')\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#############################################################\n",
    "# This code is used to determine what each groupby looks like\n",
    "#############################################################\n",
    "col = df.columns.tolist()\n",
    "col.remove('one')\n",
    "for i in col:\n",
    "  print(i)\n",
    "  do=df.groupby(i).one.sum().reset_index()\n",
    "  print(do.sort_values(by='one',ascending = False))\n",
    "  print('')\n",
    "  print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Aggregating Cleaned Data - reducing size of dataframe\n",
    "#######################################################\n",
    "col = df.columns.tolist()\n",
    "col.remove('one')\n",
    "df_agg=df.groupby(col).one.sum().reset_index()\n",
    "print(df_agg.nunique())\n",
    "print(df_agg.count())\n",
    "df_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Writing to tab separated CSV\n",
    "########################################\n",
    "'''This will write the csv file to the same\n",
    "folder as the jupyter notebook is in'''\n",
    "\n",
    "df_agg.to_csv('tableauData.csv',sep='\\t')\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
